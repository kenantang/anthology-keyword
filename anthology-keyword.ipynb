{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4394a664-cd89-4881-9869-fd49e2df5236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep, time\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc1f80fb-e856-460b-b152-7ecec1defc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dual-pivot', 'fine-tuning pair', 'shared structure', 'out-of-the-box', 'post-training', 'pivot pair', 'completely unsupervised', 'partially unsupervised', 'fourfold', 'light', 'lensing module', 'synthetic bitext', 'bootstrapped bitext', 'unnormalized score', 'degenerate', 'degraded', 'summand', 'summands', 'Arabic-English', 'German-English', 'hubness', 'hub', 'trainable components', 'to some extent', 'resource-rich', 'resource-poor', 'high-resource', 'low-resource', 'lexical constrained decoding', 'left-to-right', 'translation prefix', 'text-infilling', 'text infilling MLM', 'text infilling masked', 'hard constraints', 'maximum a posteriori', 'decoding time', 'multiple iterations', 'post-edited', 'given prefix', 'given suffix', 'burden', 'terminology constraints', 'nested NER', 'non-crossing', 'nested', 'tree fragments', 'chart-based', 'span representation', 'subspan', 'sub-span', 'cursor', 'n-ary', 'bos', 'fencepost', 'fencepost representation', 'deep biaffine function', 'boundary', 'shared boundary', 'CYK algorithm', 'CKY algorithm', 'multiple children', 'near-optimal', 'linearize', 'semi-Markov', 'shortcut', 'reorder', 'preserve order', 'hypergraph', 'headword', 'fruitful', 'flatter minima', 'flat', 'smoothed loss landscapes', 'incorrect boundary', 'sharpness', 'miscalibration', 'sharpness miscalibration', 'biaffine decoder', 'smoothing visualization', 'flat NER', 'auxilary embedding', 'over-confidence', 'over-confident', 'miscalibrated', 'reliability diagram', 'ECE', 'expected calibration error', 'loss landscape', 'geometric property', 'chaotic', 'recall-sensitive', 'translationese', 'inverse model', 'denoising auto-encoding', 'DAE', 'third-party', 'Google Translator', 'natural inputs', 'self-training', 'joint BPE', 'executable', 'Excel', 'expansion pruning', 'verbatim', 'repartitioning', 'fictional domains', 'connected component', 'accessor', 'increased latency', 'last token', 'last token embedding', 'three times', 'i.i.d.', 'bold red font', 'aggressive pruning', 'conservative pruning', 'system-generated', 'relative quality', 'one-point deterministic', 'probability correlation', 'probability correlated', 'one-point', 'reward shaping', 'quality estimation', 'two-stage', 'demonstrations', 'exploit metrics', 'novel n-grams', 'novel ngrams', 'computational overhead', 'click here', 'hyperlink', 'path-based', 'last but not least', 'high-quality negative', 'entity-to-concept', 'coarse-to-fine', 'silver standard', 'case marker', 'case polysemy', 'case homosemy', 'glossing', 'deep cases', 'loosely based', 'verse', 'Norwegian Bokm√•l', 'artefact', 'artifact', 'Fisher', 'adpositions', 'clitics', 'syncretism', 'well-generalized', '2-model ensemble', '3-model ensemble', 'ensemble-distillation', 'weight-sharing', 'alternating updates', 'gating factor', 'paired student t-test', 'ensemble variance', 'branches', 'training-from-scratch', 'low-resource datasets', 'rich-resource datasets', 'high-resource datasets', 'low variance', 'norm constraint', 'well-trained', 'API', 'APIs', 'deployment strategy', 'feature attribution', 'hand-crafted', 'semi-automatic', 'glass-box', 'binary calibration', 'null response', 'LIME and SHAP', 'SHAP and LIME', 'random forest', 'glass box', 'fool', 'union', 'utterly fail', 'utterly fails', 'utterly failed', 'in-domain OOD', 'strictly black-box', 'knowledge embedding', 'closed simile', 'open simile', 'three distractors', 'two distractors', 'usage frequencies', 'context diversity', 'context diversities', 'equally divided', 'copula', 'PCA', 't-SNE', 'knowledge-enhanced', 'equivalent prompts', 'backdoor', 'backdoor paths', 'backdoor criterion', 'structural causal model', 'causal paths', 'verbalization', 'further pretrain', 'sample disparity', 'intuitive solution', 'rank consistency', 'revise', 'revision', 'naturally occuring', 'erroneous translations', 'continued training', 'continued pre-training', 'continued pretraining', 'semantic divergence', 'local errors', 'bicleaner', 'medium-frequency words', 'rejuvenation', 'potentially idiomatic expressions', 'bottom three layers', 'top three layers', 'metonym', 'canonical correlation analysis', 'CCA', 'amnesic', 'iterative null-space projection', 'INLP', 'within-domain', 'falsify', 'clausal', 'resource-intensive', 'workaround', 'parameter-intensive', 'unlabelled finetuned', 'boundary detection', 'dialogic discourse', 'background claim', 'own claim', 'independent signals', 'unit tests', 'strategic', 'non-compositional questions', 'compositional questions', 'compositional', 'non-compositional', 'self-consistency', 'self-consistency dialog', 'invariance', 'cropping', 'populate', 'human baseline', '$', 'trip the model', '2-choice', '3-choice', 'disjunction', 'atomicity', 'human ceiling', 'BioNLP', 'Anglo-centric', 'single-human', 'community-driven', 'north star', 'search engine logs', 'real-world distribution', 'maintenance', '-STS', 'query-query', 'dishonest experts', 'utility-preserving anonymization', 'culture', 'Nordic', 'sociolects', 'colour', 'color', 'cultural assumptions', 'slurs', 'obscene words', 'geo-diverse', 'annotation projection', 'food', 'violated', 'disparity', 'group DRO', 'cross-cultural translation', 'adaptation translation', 'adaptive translation', 'decolonise', 'non-canonical', 'contingent', 'non-head words', 'collocations', 'binomial expressions', 'syntactic confounds', 'Tregex', 'nested model comparison', 'semantic idiosyncracy', 'REB approval', 'bar charts', 'line charts', 'planning-based', 'image captioning', 'sentinel token', 'bounding box positional embeddings', 'white-box', 'blunt questions', 'blunt', 'consistency tf-idf', 'TrueSkill', 'Kendall ranking correlation', 'dialog exchanges', 'in perspective', 'bot-bot', \"Grice's maxim\", 'dyadic conversations', 'low-effort', 'adversarial filtering', 'dissonance', 'multi-source training', 'many-to-many', 'saturation ensemble', 'ensemble shared layers', 'PWCCA', 'event-level', 'event-centric', 'continued pre-training BART', 'continually pre-trained BART', 'human-labeling', 'span recovering', 'recovering', 'three losses', 'four losses', 'GPU hours', 'multipart', '80/20/20', 'classic models', 'stack size', 'pilot study', 'VLU', 'prompt-generation', 'out-of-coverage', 'support set', 'optimize normalization', 'normalization optimization', 'normalization parameter', 'optimize bias', 'bias optimization', 'bias parameter', 'cheaper', 'BitFit']\n",
      "358\n"
     ]
    }
   ],
   "source": [
    "input_file = \"input.txt\"\n",
    "\n",
    "if input_file:\n",
    "    # read keywords from file\n",
    "    keywords = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            tokens = line.split('*')\n",
    "            if len(tokens) == 3:\n",
    "                keywords.append(tokens[1])\n",
    "else:\n",
    "    # use default keywords\n",
    "    keywords = ['subreddit', 'wikipedia']    \n",
    "                \n",
    "print(keywords)\n",
    "print(len(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f362159e-5d47-477e-8c41-e2a46c82d012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimize bias 43 (26.14 seconds)\n",
      "bias optimization 29 (20.16 seconds)\n",
      "bias parameter 62 (23.43 seconds)\n",
      "cheaper 42 (22.57 seconds)\n",
      "BitFit 3 (21.07 seconds)\n"
     ]
    }
   ],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# the returned number is unstable, need multiple clicks\n",
    "clicks = 5\n",
    "\n",
    "# the wait time needs to be randomized so the scraper is not identified as a bot\n",
    "# the range is uniformly sampled between [a, b] seconds\n",
    "# the time also needs to be large enough so the search results could be returned\n",
    "a = 1\n",
    "b = 1.5\n",
    "\n",
    "# the output file, which will be manually edited after obtaining all output\n",
    "fo = open(\"output.txt\", \"a\")\n",
    "\n",
    "# to start from midpoint\n",
    "start_idx = 353\n",
    "\n",
    "# with the current parameters, about 20 seconds per keyword\n",
    "for keyword in keywords[start_idx:]:\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    # random sleep for a few seconds\n",
    "    sleep(uniform(5, 10))\n",
    "\n",
    "    url = f\"https://aclanthology.org/search/?q={keyword.replace(' ', '+')}\"\n",
    "\n",
    "    # start the driver\n",
    "    driver.get(url)\n",
    "    \n",
    "    nums = []\n",
    "    \n",
    "    try:\n",
    "\n",
    "        # switch between the options multiple times\n",
    "        for _ in range(clicks):\n",
    "\n",
    "            # expand the dropdown menu\n",
    "            buttons = driver.find_elements(by=By.CLASS_NAME, value=\"gsc-selected-option-container\")\n",
    "            buttons[0].click()\n",
    "\n",
    "            # choose year of publication\n",
    "            buttons =  driver.find_elements(by=By.CLASS_NAME, value=\"gsc-option-menu-item\")\n",
    "            buttons[1].click()\n",
    "\n",
    "            # some time is needed for the result to change\n",
    "            sleep(uniform(a,b))\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            # result in the format \"About 31 results (0.11 seconds)\"\n",
    "            result = soup.find_all(\"div\", {\"class\": \"gsc-result-info\"})\n",
    "\n",
    "            # get the number\n",
    "            try:\n",
    "                num = result[0].string.split(' ')[1].replace(',', '')\n",
    "            except:\n",
    "                num = 0\n",
    "            nums.append(num)\n",
    "\n",
    "            # expand the dropdown menu\n",
    "            buttons = driver.find_elements(by=By.CLASS_NAME, value=\"gsc-selected-option-container\")\n",
    "            buttons[0].click()\n",
    "\n",
    "            # change back to relevance\n",
    "            buttons =  driver.find_elements(by=By.CLASS_NAME, value=\"gsc-option-menu-item\")\n",
    "            buttons[0].click()\n",
    "            sleep(uniform(a,b))\n",
    "\n",
    "        end = time()\n",
    "\n",
    "        print(keyword, min(nums), f\"({end-start:.2f} seconds)\")\n",
    "\n",
    "        fo.write(f\"{keyword}, {min(nums)}\\n\")\n",
    "        fo.flush()\n",
    "        \n",
    "    except:\n",
    "        print(f\"The {keywords.index(keyword)}-th keyword {keyword} failed!\")\n",
    "        break\n",
    "\n",
    "# quit the driver\n",
    "driver.quit()\n",
    "\n",
    "# close the file\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8611c4e8-b782-4d0a-bf51-80d79faeb7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
