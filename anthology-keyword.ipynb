{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4394a664-cd89-4881-9869-fd49e2df5236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep, time\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc1f80fb-e856-460b-b152-7ecec1defc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['incomplete labelling', 'positive and unlabelled', 'PU', 'classification risk', 'Rademacher', 'Rademacher complexity', 'covariate shift', 'seasonal', 'time-varying covariates', 'demographics', 'family-wise', 'pre-collected', 'unanswerable', 'feature cluster', '100 classes', 'confirmation bias', 'loss correction', 'cluster-level', 'sieve', 'sieving', 'stable hyperparameters', 'unstable hyperparameters', 'hierarchical loss', 'quality estimation', 'noisy learning', 'trusted instances', 'trusted instance', 'contexual parameters', 'multi-encoder regularization', 'contrastive test sets', 'contrastive translation', 'contrastive evaluation', 'contrastive ranking', 'contrastive rank', 'along this path', 'imputed', 'NDO', 'non-domunant order', 'German NDO', 'native language', 'native language effect', 'ABX discriminability', 'metamers', 'reduced sensitivity', 'contrastive predicting coding', 'fifth layer', 'probit regression', 'phone contrasts', 'acoustic scenes', 'non-speech', 'biologically plausible', 'attention vector entropy', 'attention flow', 'task-modulated', 'task-neutral', 'untuned', 'overcomplete', 'hard-to-predict', 'from-scratch', 'orientation', 'domain orientation', 'orientation vector', 'ADA', 'any domain adaptation', 'pivot features', 'subtrahend', 'weighted BCE loss', 'BCE loss', 'query style', 'style query', 'runner-up', \"annotators' behaviors\", 'annotator behavior', 'edit-based', 'edit-based annotation', 're-annotated', 're-annotate', 'language discriminator', 'gradient reversal', 'reversal', 'Sent2Vec', 'capsule networks', 'confidence penalty', 'confidence penalty loss', 'activation boundary', 'entity anonymization', 'anonymization', 'non-parametric', 'crosswordese', 'rebus', 'MIPS', 'Satisfiability Modulo Theories', 'snippet', 'snippet-level', 'receptive field', 'consistency loss', 'dynamic weights', 'top-K parameterize', 'top-K parameter', 'top-K dynamic weights', 'locator', 'hyperparameter binary search', 'binary search', '2-step binary search', '3-step binary search', 'performant', 'trigger', 'world event knowledge', 'pair constraints', 'pair constraint', 'memory store', 'demonstrations', 'dynamic decoding', 'dynamic prefix', 'data-efficient', 'irrelevance classifier', 'membership', 'evolving', 'warm-up proportion', 'free-text-formed', 'reporting bias', 'superficial cues', 'causal-strength', 'FGM', 'swift models', 'super models', 'instance-wise', 'negative free energy', 'free energy', 'negative free', 'reassembling', 'ghost modules', 'ghost', 'multi-exit', 'EBM', 'artificial token', 'noise ratio', 'curious', 'class distribution similarity', 'class distribution similarities', 'local intrinsic dimensionality', 'intrinsic dimensionality', 'Elec', 'over-training', 'cognitive demand', 'distribution learning', 'guided conversation', 'silver', 'control signals', 'E2E', 'word-of-mouth', 'childcare', 'niche-targeting', 'nichetargeting', 'iteratively refine', 'structured sentiment analysis', 'SSA', 'POS embedding', 'part-of-speech embedding', 'lemma embedding', 'rotary position embedding', 'Circle loss', 'cosine annealing warm restarts', 'kNN retrieval', 'compact network', 'datastore', 'Meta-k', 'Meta-k network', 'one-plus-one', 'entropy-based pruning', 'DBSCAN', 'BIRCH', 'pivot selection', 'random pruning', 'pruning rate', 'top-performed', 'top-performing', 'stable convergence', 'validation curvature', 'time budget', 'nuclear norm', 'inverse relation', 'add inverse relation', 'HP', 'HPs', 'SRCC', 'curvature', 'sample subgraphs', 'subgraph sampling', 'decouple search space', 'quasi-random search', 'robuster', 'sentence compression', 'look-ahead', 'look-ahead mechanism', 'moving average', 'dense annotations', 'information tree', 'temporal grounding', 'one-shot', 'semantic relevance', 'Intersectin over Union', 'IoU', 'surgically remove', 'ASTE', 'biaffine attention', 'multi-channel', 'classifier chains', 'BERTwwm', 'BERT-wwm', 'enhancement module', 'multi-step information propagation', 'MSIP', 'MixPooling', 'heterogeneous graph', 'betweenness centrality', 'concretization', 'thulac', 'ictclas', 'hanlp', 'elbow', 'lattice', 'full-shot', 'TPU Google Colab', 'mixed prompting', 'right-to-left re-ranking', 'Parallel Iterative Edit', 'Learner English', 'NUCLE', 'fast tokenizers', 'quorum', 'majority quorum', 'errorful', 'distributional similarity', 'distributional inclusion hypothesis', 'balanced inclusion', 'neo-Davidsonian', 'neo-Davisonian', 'sentence generator', 'Gödel t-norm', 'Gödel probability', 'centering theory', 'AWQ', 'entity grid', 'depthwise convolution', 'depthwise', 'depth-wise', 'Tree-Transformer', 'one-sample t-test', 'two-sample t-test', 'premarked', 'premark', 'novel unigrams', 'novel bigrams', 'proverb', 'surface overlap', 'overfit to annotator', 'VADER', 'VADER sentiment', 'bind', 'recombine', 'recombined', 'out-of-context', 'tree edit distance', 'running example', 'I will', 'physiognomy', 'buy-in', 'boiler-plate', 'semi-standardized', 'conductance', 'recourse', 'Green AI', 'ethics washing', 'information contrast model', 'MAAC', 'macro average accuracy', 'category proximity', 'adverse events', 'one-error', 'propensity', 'extreme classification', 'mono-label', 'information content', 'statistical specificity', 'similarity axioms', 'linear contrast model', 'a taxonomy of errors', 'impenetrable', 'medical simplification', 'paucity', 'codify', 'decontextualization', 'word-to-word', 'further adaptation', 'scraping-bys', 'PanLex', 'eflomal', 'induced lexicon', 'graph perturbations', 'concatenated edges', 'graph autoregressive', 'autoregressive graph', 'tree autoregressive', 'autoregressive tree', 'generate-and-refine', 'historical use', 'Tree-BertScore', 'Maximum Input Unit', 'IME', 'Chinese IME', 'online vocabulary adaptation', 'glyph', 'pinpointing', 'mention-entity', 'Xhinua', 'classy', 'in the same ballpark', 'historical analyses', 'Args.me', 'reduction factor', 'text blocks', 'a maximum length of 128', 'laypersons', 'superficial competitiveness', 'ERM', 'MiniLMv2', 'worst class influence', 'worst-group performance', 'ballpark', 'Adversarial Removal', 'IRM', 'V-REx', 'proof graph', 'unseen subjects', 'continued concatenation', 'stitched', 'inference time', 'forward chaining', 'operating points', 'faithfulness errors', 'control models', 'trade-off curve', 'XSum hallucinations', 'selector', 'loss truncation', 'education domain', 'sub-skills', 'unconventional punctuation', 'focal event', 'Cronbach', \"Cronbach's coefficient alpha\", 'entropy statistics', 'artificial development set', 'order sensitivity', 'block n-gram repetitions', 'paid-for', 'trigger tokens', 'lexical bias', 'unrealistic information', 'LMI', 'presupposition', 'college student', 'data whitening', 'readability metrics', 'semantic understanding', 'class-incremental', 'new classes', 'old classes', 'replay dataset', 'data-free distillation', 'non-special tokens', 'code-mixing addition', 'scalar knob', 'writing assistance', 'inference-time trick', 'inference trick', 're-insertion', 'stop-grad', 'stop gradient', '3-point scale', 'LaBSE', 'dataless classification', 'SN', 'batch softmax', 'in-batch negatives', 'acceptability', \"Welch's t-test\", 'Welch', 'cross attention', 'NVIDIA Triton', 'heterogeneous knowledge', 'conflicting knowledge', 'label description', 'label descriptions', 'in-house', 'self-describing', 'nutrition', 'banking', 'information gain', 'DBPedia Spotlight', 'gradient-boosting', 'dev-test correlation', 'summary of findings', 'K-fold', 'minimum description length', 'bagging', 'model informed', 'leave-of-out', 'unstable variance', 'semi-supervised few-shot', 'VaTeX', 'fill-in-the-blank', 'fill-in-the-blanks', 'extra spaces', 'early-fusion', 'late-fusion', 'two-stream', 'spatial structure', 'single-stream', 'dual-stream', 'scene tree', 'scene grammar', 'scene graph', 'UUAS', 'undirected unlabelled attachment score', 'fluctuate inconsistently', 'informativeness distribution', \"Matthew's correlation\", 'stockpile', 'stockpiles', 'word inclusion', 'non-residual', 'non-residual attention', 'causal self-attention', 'mean offset', 'XBRL', 'subword fragmentation', 'token shape', 'token magnitude', 'IOB2', 'Bloom embeddings', 'Bloom embedding', 'last-pooling', 'structural statistics', 'burn-in', 'Prolific', 'benepar', 'Gibbs sampler', 'Gibbs sampling', 'legit', 'data selection', 'flooding', 'one backward pass', 'accordance', 'accordant', 'gradient accordance', 'sub-epoch', 'stiffness', 'well-received', 'normalizing flows', 'latent representation transformation', 'location-scale', 'BIOES', 'curly Iverson brackets', 'Iverson', 'beta skeleton graph', 'beta skeleton', 'skeleton', 'noisy serialization', 'serialization', 'dimension inference', 'unit inference', 'index inference', 'Enron', 'sketch', 'table pre-training', 'stereotyping', 'MSc', 'double switch', 'citizen science', 'untranslatable', 'minimal pair', 'US culture', 'U.S. culture', 'idiomacy', 'social frames', 'Social Frames', 're-translation', 'wait-k', 'uncover', 'top-of-the-line', 'non-interactive', 'informal conversation', 'monologic', 'ascription', 'reified', 'continuer', 'Estoup', 'prima facie', 'turbulence', 'repair initiator', 'empirical grounding', 'Appen', 'internalize', 'hyperbolic space', 'tangent space', 'dependency tree probing', 'Frechet mean', 'Lorentz centroid', 'Einstein midpoint', 'floating-point error', 'half precision training', 'top-notch', 'predicate-argument', 'node merging', 'graph-to-text', 'text-to-graph', 'smatch', 'espeto', 'wash away', 'self-talk', 'product-of-experts', 'PoE', 'engineering-efficient', 'cache-based', 'long-form generation', 'Markov logic networks', 'citation', 'z-statistics', 'unlikelihood', 'stress test', 'atomic', 'atomic assertions', 'citances', 'citance', 'check-worthy', 'entity-replacement', 'scientific NLP', 'metathesaurus', 'overpredict', 'AIDA', 'AIDA framework', 'personalization', 'word-to-sequence', 'Academic Vocabulary List', 'OOTB', 'API limits', 'PPLM', 'Thing Explainer', 'eschew', 'bottleneck', 'bottlenecked', 'generative conditioning', 'autoencoding', 'pattern-exploiting training', 'apple-to-apple', 'foreign-language country', 'Austronesian', 'local entities', 'auxilary supervision', 'PTLM', 'PTLMs', 'embedder', 'cookie theft', 'within-set', 'paired perplexity', 'Celex Lexical Database', 'LDC96L14', 'log frequency bands', 'TTR', 'ecco', 'non-key', 'subword graph', 'Indiana University', 'secondary information', 'focus-attention', 'saliency-selection', 'graph self-supervised training', 'graph pre-training', 'ego-networks', 'ego network', 'protein ego-networks', 'tree pre-training', 'document rotation', 'Gigaword', 'LDC2011T07', 'reentrancy', 'AMR tree', 'irregular', 'time-series', 'guided cross-attention', 'time vector', 'value-at-risk', 'GDELT', 'Pyramid method', 'worthy', 'SimAlign', 'subtree information', 'Eisner', 'Eisner algorithm', 'parsing-as-deduction', 'spurious ambiguity', 'hook trick', 'head-splitting trick', 'bookkeeping', 'cost-augmented inference', 'batchify', 'batchify pytorch', 'belief propagation', 'soft trees', 'meta relations', 'IRC', 'graph-to-sequence', 'message passing', 'document structure', 'document structures', 'hierarchical biases', 'roundtrip consistency', 'iterative refinements', 'sketch', 'hierarchical set', 'residual error', 'depth dropout', 'cluster centroids', 'recursive k-means', 'hierarchical indexing', 'well-handle', 'Statistics Canada', 'StatCan', 'phrase deletion', 'lisp', 'lisp interpreter', 'spurious program rate', 'ToTTo', 'spatial prompts', 'quadrant', 'hot-spots', 'VisualGenome', 'affinity score', 'affinity scores', 'unimodal', 'neocolonialism', 'community-based', 'polysynthesis', 'decolonize', 'SIGEL', 'playfulness', 'game with a purpose', 'GWAP', 'colorful pictures', 'children books', 'non-linguistic', 'non-linguistic input', 'MQM', 'function words', 'interpolate', 'functional structure', 'functional roles', 'hypophora', 'SOAP', 'patient-nurse', 'patient-nurse summaries', 'discharge summaries', 'discharge', 'stylistic differences', 'SNOMED', 'logical semantics', 'overgeneration', 'planner', 'UCT', 'branching factor', 'OLIVE', 'complementizer', 'FOL', 'intra-associations', 'directed hypergraph', 'graph walk', 'path annotation', 'over-smoothing', 'hyperlink', 'click-logs', 'topical entity', 'similarity between corpus', 'AI debate', 'argument mining', 'sentence-pair classification', 'table-filling', 'mixup', 'HarvestText', 'parameter generator network', 'PGN', 'beta distribution', 'OEI', 'ideation', 'color-coded', 'meta-features', 'geographical cultures', 'at hand', 'defeasible logic', 'hotlist', 'trivial', 'difficulty scores', 'item response theory', 'IRT', 'pragmatics', 'minimally contrastive', 'BRISQUE score', 'average dependency tree depth', 'human body parts', 'highway layer', 'explanatory variables', 'modelling negation', 'timestamps', 'code clone', 'clone', 'creative language', 'metonymy', 'figurative paraphrase', 'compositional generalization', 'template engine', 'repurpose', 'BLEURT self-training', 'prototype-based', 'neural tensor networks', 'Ollie', 'testing concept activation vector', 'TCAV', 'Founta', 'boosted sampling', 'concept activation vector', 'CAV', 'implicit abuse', 'explicit abuse', 'multiple teachers', 'sequential transfer learning', 'weakly-labeled', 'weakly-labelled', 'imitation', 'query sets', 'construction loss', 'label correlation', 'Offensive Language Identification Dataset', 'OLID', 'ad hominem', 'taxonomy tree', 'taxonomy tree position', 'multi-faceted CRF', 'visual tokens', 'patch', 'patch-level', 'embedding codebook', 'fast clustering', 'questionnaire', 'PHQ9', 'clinical instrument', 'Beck Depression Inventory', 'micromodel', 'extra neuron', 'anhedonia', 'complex-value embedding', 'ComplEx', 'modus operandi', 'Advanced Mapping', 'Generalized Procrustes', 'BLI', 'mimick-like', 'typo', 'hard negative generation', 'Graphormer', 'HTC', 'NT-Xent', 'stance detection', 'efficient market hypothesis', 'perculiarities', 'cognitive distortion', 'TextBlob', 'HateSonar', 'multi-hot', 'unconstrained', 'contextual bandit', 'option set', 'Pyro', 'natural prior', 'adaptation-networks', 'circumflex', 'technology burnout', 'approximate nearest neighbor', 'beam blocking', 'Bing search', 'lemon picked', 'siloed', 'Gestalt pattern matching', 'orthographic depth', 'synesthetes', 'jamos', 'jamo', 'representational similarity analysis', 'RSA', 'logography', 'ipapy', 'CIELuv', 'Bonferroni correction', 'binding theory', 'chart-based', 'non-local', 'paraphrase detection', 'Shapley', 'effective attention', 'frequency penalty', 'presence penalty', 'length embedding', 'noun chunks', 'semi-structured', 'prototypical graph', 'edge-oriented', 'statistically stable', 'Spreading-Activation Theory', 'relatedness', 'l2 similarity', 'Gaussian distance', 'fuzzy actions', 'fuzzy operations', 'fuzzy grammar', 'two-layer', 'fuzzy comparison', 'multi-channel', 'single character words', 'contrast vanishing', 'TextRank', 'Mahalanobis', 'Mahalanobis distance', 'geometrical disparity', 'imposter', 'one-off', 'translation noise', 'morphological alternations', 'non-concatenative morphology', 'two-tier', 'Bantu', 'Bantu languages', 'desktop machine', 'compact region', 'simply-connected region', 'simply-connected', 'Gaussian Hypothesis Testing', 'open space risk', 'counseling', 'soft positional encoding', 'Zenserp', 'position collision', 'Qualtrics', 'integrated gradients', 'Riemann approximation', 'programming language', 'path search', 'secondary pre-training', 'span denoising', 'fewer gradient updates', 'Hungarian algorithm', 'shortlist', 'noisy matching', 'gumbel-sinkhorn', 'systematic generalization', 'positional embedding downscaling', 'restart mechanism', 'poset', 'TLM', 'ranking loss', 'constrastive-data-selection', 'cross-accelerator', 'progressive stacking', 'm-USE', 'additive margin', 'vocoder', 'vocoders', 'proprietary', 'meta-parameters', 'phonemizer', 'second order derivatives', 'guided attention loss', 'phonetically balanced', 'inverted index mapping', 'Louvain method', 'Louvain', 'scientific WSI', 'SPIKE', 'aggravate', 'exponential moving average', 'Moses', 'inverse square root', 'inverse square root learning rate', 'non-autoregressive transformer', 'glancing']\n",
      "894\n"
     ]
    }
   ],
   "source": [
    "input_file = \"input.txt\"\n",
    "\n",
    "if input_file:\n",
    "    # read keywords from file\n",
    "    keywords = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            tokens = line.split('*')\n",
    "            if len(tokens) == 3:\n",
    "                keywords.append(tokens[1])\n",
    "else:\n",
    "    # use default keywords\n",
    "    keywords = ['subreddit', 'wikipedia']    \n",
    "                \n",
    "print(keywords)\n",
    "print(len(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f362159e-5d47-477e-8c41-e2a46c82d012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-autoregressive transformer 69 (23.85 seconds)\n",
      "glancing 26 (20.84 seconds)\n"
     ]
    }
   ],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# the returned number is unstable, need multiple clicks\n",
    "clicks = 5\n",
    "\n",
    "# the wait time needs to be randomized so the scraper is not identified as a bot\n",
    "# the range is uniformly sampled between [a, b] seconds\n",
    "# the time also needs to be large enough so the search results could be returned\n",
    "a = 1\n",
    "b = 1.5\n",
    "\n",
    "# the output file, which will be manually edited after obtaining all output\n",
    "fo = open(\"output.txt\", \"a\")\n",
    "\n",
    "# to start from midpoint, CHANGE THIS!\n",
    "start_idx = 892\n",
    "\n",
    "# with the current parameters, about 20 seconds per keyword\n",
    "for keyword in keywords[start_idx:]:\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    # random sleep for a few seconds\n",
    "    sleep(uniform(5, 10))\n",
    "\n",
    "    url = f\"https://aclanthology.org/search/?q={keyword.replace(' ', '+')}\"\n",
    "\n",
    "    # start the driver\n",
    "    driver.get(url)\n",
    "    \n",
    "    nums = []\n",
    "    \n",
    "    try:\n",
    "\n",
    "        # switch between the options multiple times\n",
    "        for _ in range(clicks):\n",
    "\n",
    "            # expand the dropdown menu\n",
    "            buttons = driver.find_elements(by=By.CLASS_NAME, value=\"gsc-selected-option-container\")\n",
    "            buttons[0].click()\n",
    "\n",
    "            # choose year of publication\n",
    "            buttons =  driver.find_elements(by=By.CLASS_NAME, value=\"gsc-option-menu-item\")\n",
    "            buttons[1].click()\n",
    "\n",
    "            # some time is needed for the result to change\n",
    "            sleep(uniform(a,b))\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            # result in the format \"About 31 results (0.11 seconds)\"\n",
    "            result = soup.find_all(\"div\", {\"class\": \"gsc-result-info\"})\n",
    "\n",
    "            # get the number\n",
    "            try:\n",
    "                num = result[0].string.split(' ')[1].replace(',', '')\n",
    "            except:\n",
    "                num = 0\n",
    "            nums.append(num)\n",
    "\n",
    "            # expand the dropdown menu\n",
    "            buttons = driver.find_elements(by=By.CLASS_NAME, value=\"gsc-selected-option-container\")\n",
    "            buttons[0].click()\n",
    "\n",
    "            # change back to relevance\n",
    "            buttons =  driver.find_elements(by=By.CLASS_NAME, value=\"gsc-option-menu-item\")\n",
    "            buttons[0].click()\n",
    "            sleep(uniform(a,b))\n",
    "\n",
    "        end = time()\n",
    "\n",
    "        print(keyword, min(nums), f\"({end-start:.2f} seconds)\")\n",
    "\n",
    "        fo.write(f\"{keyword}, {min(nums)}\\n\")\n",
    "        fo.flush()\n",
    "        \n",
    "    except:\n",
    "        print(f\"The {keywords.index(keyword)}-th keyword {keyword} failed!\")\n",
    "        break\n",
    "\n",
    "# quit the driver\n",
    "driver.quit()\n",
    "\n",
    "# close the file\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8611c4e8-b782-4d0a-bf51-80d79faeb7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
