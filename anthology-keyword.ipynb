{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4394a664-cd89-4881-9869-fd49e2df5236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep, time\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc1f80fb-e856-460b-b152-7ecec1defc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heavy tail', 'running example', 'synthetic tasks', 'side information', 'tabular', 'qualification quiz', 'HIT', 'hit', '1-4 scale', '1-5 scale', 'Flesch', 'linearization', 'homogeneous', 'Hitlin', 'cold start', 'confidentiality', 'expertise', 'inter-training', 'cluster curriculum', 'curriculum cluster', 'stylistic distinction', 'sequential information bottleneck', 'sIB', 'normalized mutual information', 'NMI', 'pseudo-label MLM', 'pseudo-label BERT', 'key-point analysis', 'sparse patterns', 'locality sensitive hashing', 'LSH', 'partitioning', 'fuzzy clustering', 'blocks', 'finite experiments', 'task taxonomy', 'brain', 'neuroscience', 'brain activity', 'analytic hierarchy process', 'cognitive', 'exhaustive', 'transductive TL', 'inductive TL', 'lifelong learning', 'task embedding', 'Canberra distance', 'non-target', 'voxel', 'hierarchical clustering', 'long range', 'over-sensitive', 'oversensitive', 'recency bias', 'tempered sampling', 'log-linear boosting', 'undertraining', 'single pass', 'hyposensitivity', 'soft matching', 'online storage', 'codebook-based', 'decompression', 'footprint', 'overhead', 'scalar quantization', 'codebook compression', 'softplus', 'asymmetric distance computation', 'precomputed', 'product quantizer', 'additive quantizer', 'preloaded', 'standalone', 'entire set', 'SFT', 'lottery ticket', 'rewind', 'model-independent', 'model-agnostic', 'Fisher information', 'derogation', 'lottery ticket selection', 'lottery ticket winning', 'compositionality', 'compound divergence', 'SPARQL', 'compressed SPARQL', 'arc', 'dependency arc', 'full-precision', 'historical linguistics', 'hand-annotated', 'sporadically', 'treebank', 'treebanking', 'desideratum', 'desiderata', 'standardised', 'standardized', 'IAST', 'IPA', 'tonogenesis', 'toneme', 'tonemes', 'areal', 'areal studies', 'subtrate language', 'vociferous', 'Transkribus', 'mutual information', 'bilingual mutural information', 'token-level', 'sentence-level', 'up-weight', 'mixed precision', 'synchronously', 'over-focus', 'self-paced', 'extra data', 'yes/no', 'not-answerable', 'unanswerable', 'decouple', 'public policy', 'HTML', 'HTML elements', 'snippet', 'ETC', 'common error', 'common errors', 'L2R', 'R2L', 'relatively low probabilities', 'threshold', 'partially observable', 'pertinently', 'backward network', 'backward decoder', 'unconfident', 'unconfidently', 'bridging', 'bridging resolver', 'cross-task', 'learning-based', 'soft constraints', 'element-wise multiplication similarity', 'hardness coefficient', 'dummy', 'precision-oriented', 'recall-oriented', '10-fold', 'harsh', 'max-margin', 'Gaussian-distributed embeddings', 'Gaussian embeddings', 'prototypical networks', 'variance estimation', 'point embedding', 'numerical stability', '5-shot', '1-shot', 'ProtoBERT', 'uniformity', 'support data', 'micro-context', 'BERTology', 'Gricean', 'unsaturated', 'pioneering', 'misprime', 'multi-tower', 'dual-encoder', 'avoid saturation', 'leakage', 'query-context', 'fine-to-coarse', 'distributional semantics', 'BERT-small', 'token alignment', 'embedding alignment', 'cosine similarity temperature', 'SSL', 'fake tokens', 'plug-and-play', 'WordSim353', 'disparate', 'unseen', 'minimal modification', 'embedding space regularization', 'embedding regularization', 'open-vocabulary', 'hallmark', 'siamese', 'multi-margin', 'pairwise margin', 'memory data', 'lower bound', 'upper bound', 'quantized vector', 'fuse', 'syntax-aware', 'formulas', 'triplet completion', 'operator trees', 'OPT', 'multi-view', 'bi-directional knowledge transfer', 'rehearsal', 'replay', 'service', 'abort', 'abruptly', 'experience replay', 'progressive neural network', 'compositional modules', 'dissimilar', 'anisotropy', 'inner layers', 'self-similarity', 'swiss-roll', 'swiss-roll manifold', 'embedding geometry', 'semantic geometry', 'BOS token', 'CWE', 'CWEs', 'readability', 'control tokens', 'discrete control tokens', 'continuous control tokens', 'word coverage', 'GeForce', 'disjointed', 'constrain', 'constrains', 'soft control', 'stochastic search', 'MaxPool', 'kth', 'k-th', 'maxout', 'MindSpore', 'community use', 'Standard Roman Orthography', 'SRO', 'child-directed', 'community-based', 'EGIDS', 'SRO-syllabics converter', 'SRO-syllabics', 'BoW format', 'insert a space', 'normative effects', 'computer assisted language learning', 'CALL', 'SoundHunters', 'constituent order', 'WALS', 'polynomial decay', 'constituent shuffle', 'word order', 'cross-lingual distantly-supervised', 'type hierarchy', 'ultra-fine', 'FGET', 'bilingual lexicon induction', 'phrase retrieval', 'momentum encoder', 'Elasticsearch', 'VecMap', 'fine-grained discrete', 'localization', 'two-branch', 'SHN', 'NCE', 'MMS', 'InfoNCE', 'codebook', 'codeword', 'softmin', 'median rank', 'trajectory', 'sizable gap', 'headroom', 'as-is', 'prosody', 'prosody variation', 'style token', 'frame-to-phoneme', 'energy', 'pitch and energy', 'phoneme transformer', 'mixture encodings', 'text infilling evaluation', 'attribute relevance', 'NISF', 'normalized inverse sentence frequency', 'verbalizer', 'evaluator set', 'Pearson Spearman Kendall', 'model drift', 'good-enough', 'good enough', 'langid', 'Bhattacharyya', 'Bhattacharyya coefficient', 'socioeconomic factors', 'lossy', 'RBO', 'ranked biased overlap', 'generative-based', 'deconfounding', 'unfastened', 'centering model', 'generate negative examples', 'AMR', 'AMR graph', 'logical flaws', 'logical flaw', 'cone', 'whitening', 'sampling bias', 'word class', 'EEG', 'confounding dimensions', 'weblogs', 'RSVP', 'Rapid Serial Visual Representation', 'PsychoPy', 'Flax/Jax', 'False Discovery Rate', 'FDR', 'butterfly plot', 'butterfly plots', 're-averaging', 'Wilcoxon signed rank-test', 'train-test overlap', 'hash collision', 'bloom filter', 'suffix array', 'MinHash', 'edit similarity', 'ILP', 'integer linear programming', 'inductive logic programming', 'reasoner', 'reasoner module', 'biGRU', 'existential predicate', 't-norm', 'gradient flow', 'invented predicate', 'heuristic noise', 'surface form', 'fairseq noise', 'fairseq noise function', 'continue finetuning', 'multi-task finetuning', 'span-span', 'subtree-subtree', 'left-out', 'expressivity', 'large margin objective', 'third-order', 'MST decoding', 'thorny', 'delexicalize', 'MoS', 'unwieldy', 'newswire', 'unassimilated', 'anglicism', 'anglicisms', 'Doccano', 'sense2vec', 'blenderbot finetune', 'KGAT', 'kernel graph attention network', 'proverbial', 'future predicting', 'n-stream', 'stream', 'Free Bits', 'full manual', 'entirely manual', 'gold-quality', 'synset', 'archaic usage', 'archaic usages', 'monosemous', 'polysemous verb', 'highly polysemous', 'search errors', 'model errors', 'search errors model errors', 'cascade structure', 'summary paraphrase', 'MARL', 'centralized training', 'decentralized execution', 'coverage mechanism', 'channel', 'joint actions', 'AutoPhrase', 'denovo', 'denovo training', 'de novo training', 'training from scratch', 'board-certified', 'topic fusion', 'expert-derived', 'inference pass', 'unwritten languages', 'unwritten', 'speech units', 'S2ST', 'connectionist temporal classification', 'CTC', 'over-generation', 'vocoder', 'cepstral', 'cepstrum', 'telephone', 'filterbank', 'mean opinion scores', 'MOS', 'timeit', 'PyPAPI', 'memory-profiler']\n",
      "430\n"
     ]
    }
   ],
   "source": [
    "input_file = \"input.txt\"\n",
    "\n",
    "if input_file:\n",
    "    # read keywords from file\n",
    "    keywords = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            tokens = line.split('*')\n",
    "            if len(tokens) == 3:\n",
    "                keywords.append(tokens[1])\n",
    "else:\n",
    "    # use default keywords\n",
    "    keywords = ['subreddit', 'wikipedia']    \n",
    "                \n",
    "print(keywords)\n",
    "print(len(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f362159e-5d47-477e-8c41-e2a46c82d012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairseq noise function 0 (25.53 seconds)\n",
      "continue finetuning 62 (21.63 seconds)\n",
      "multi-task finetuning 280 (19.55 seconds)\n",
      "span-span 980 (22.28 seconds)\n",
      "subtree-subtree 104 (22.67 seconds)\n",
      "left-out 51 (22.95 seconds)\n",
      "expressivity 40 (23.33 seconds)\n",
      "large margin objective 21 (25.05 seconds)\n",
      "third-order 204 (21.72 seconds)\n",
      "MST decoding 2 (21.16 seconds)\n",
      "thorny 5 (21.89 seconds)\n",
      "delexicalize 2 (20.32 seconds)\n",
      "MoS 69 (23.79 seconds)\n",
      "unwieldy 3 (24.42 seconds)\n",
      "newswire 155 (23.89 seconds)\n",
      "unassimilated 1 (22.06 seconds)\n",
      "anglicism 2 (21.17 seconds)\n",
      "anglicisms 2 (19.51 seconds)\n",
      "Doccano 2 (23.74 seconds)\n",
      "sense2vec 0 (20.71 seconds)\n",
      "blenderbot finetune 0 (19.25 seconds)\n",
      "KGAT 1 (20.80 seconds)\n",
      "kernel graph attention network 2 (20.14 seconds)\n",
      "proverbial 1 (21.98 seconds)\n",
      "future predicting 72 (20.42 seconds)\n",
      "n-stream 43 (22.20 seconds)\n",
      "stream 638 (22.44 seconds)\n",
      "Free Bits 4 (23.05 seconds)\n",
      "full manual 69 (21.35 seconds)\n",
      "entirely manual 41 (24.70 seconds)\n",
      "gold-quality 297 (19.27 seconds)\n",
      "synset 107 (22.49 seconds)\n",
      "archaic usage 0 (19.20 seconds)\n",
      "archaic usages 0 (23.01 seconds)\n",
      "monosemous 9 (21.58 seconds)\n",
      "polysemous verb 46 (20.90 seconds)\n",
      "highly polysemous 65 (20.91 seconds)\n",
      "search errors 2990 (22.64 seconds)\n",
      "model errors 1690 (23.13 seconds)\n",
      "search errors model errors 1740 (21.49 seconds)\n",
      "cascade structure 12 (24.29 seconds)\n",
      "summary paraphrase 5 (21.61 seconds)\n",
      "MARL 5 (24.25 seconds)\n",
      "centralized training 7 (20.75 seconds)\n",
      "decentralized execution 0 (19.59 seconds)\n",
      "coverage mechanism 104 (23.14 seconds)\n",
      "channel 460 (24.71 seconds)\n",
      "joint actions 95 (21.33 seconds)\n",
      "AutoPhrase 0 (23.32 seconds)\n",
      "denovo 1 (22.34 seconds)\n",
      "denovo training 0 (19.66 seconds)\n",
      "de novo training 0 (21.11 seconds)\n",
      "training from scratch 121 (23.64 seconds)\n",
      "board-certified 1 (21.97 seconds)\n",
      "topic fusion 18 (20.66 seconds)\n",
      "expert-derived 36 (21.67 seconds)\n",
      "inference pass 28 (22.94 seconds)\n",
      "unwritten languages 12 (21.68 seconds)\n",
      "unwritten 23 (24.01 seconds)\n",
      "speech units 2250 (24.36 seconds)\n",
      "S2ST 6 (19.72 seconds)\n",
      "connectionist temporal classification 20 (23.78 seconds)\n",
      "CTC 80 (23.27 seconds)\n",
      "over-generation 785 (21.98 seconds)\n",
      "vocoder 3 (24.07 seconds)\n",
      "cepstral 20 (23.89 seconds)\n",
      "cepstrum 4 (23.72 seconds)\n",
      "telephone 253 (20.91 seconds)\n",
      "filterbank 3 (21.73 seconds)\n",
      "mean opinion scores 8 (22.98 seconds)\n",
      "MOS 69 (24.12 seconds)\n",
      "timeit 55 (20.12 seconds)\n",
      "PyPAPI 0 (21.37 seconds)\n",
      "memory-profiler 0 (20.91 seconds)\n"
     ]
    }
   ],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# the returned number is unstable, need multiple clicks\n",
    "clicks = 5\n",
    "\n",
    "# the wait time needs to be randomized so the scraper is not identified as a bot\n",
    "# the range is uniformly sampled between [a, b] seconds\n",
    "# the time also needs to be large enough so the search results could be returned\n",
    "a = 1\n",
    "b = 1.5\n",
    "\n",
    "# the output file, which will be manually edited after obtaining all output\n",
    "fo = open(\"output.txt\", \"a\")\n",
    "\n",
    "# to start from midpoint, CHANGE THIS!\n",
    "start_idx = 356\n",
    "\n",
    "# with the current parameters, about 20 seconds per keyword\n",
    "for keyword in keywords[start_idx:]:\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    # random sleep for a few seconds\n",
    "    sleep(uniform(5, 10))\n",
    "\n",
    "    url = f\"https://aclanthology.org/search/?q={keyword.replace(' ', '+')}\"\n",
    "\n",
    "    # start the driver\n",
    "    driver.get(url)\n",
    "    \n",
    "    nums = []\n",
    "    \n",
    "    try:\n",
    "\n",
    "        # switch between the options multiple times\n",
    "        for _ in range(clicks):\n",
    "\n",
    "            # expand the dropdown menu\n",
    "            buttons = driver.find_elements(by=By.CLASS_NAME, value=\"gsc-selected-option-container\")\n",
    "            buttons[0].click()\n",
    "\n",
    "            # choose year of publication\n",
    "            buttons =  driver.find_elements(by=By.CLASS_NAME, value=\"gsc-option-menu-item\")\n",
    "            buttons[1].click()\n",
    "\n",
    "            # some time is needed for the result to change\n",
    "            sleep(uniform(a,b))\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            # result in the format \"About 31 results (0.11 seconds)\"\n",
    "            result = soup.find_all(\"div\", {\"class\": \"gsc-result-info\"})\n",
    "\n",
    "            # get the number\n",
    "            try:\n",
    "                num = result[0].string.split(' ')[1].replace(',', '')\n",
    "            except:\n",
    "                num = 0\n",
    "            nums.append(num)\n",
    "\n",
    "            # expand the dropdown menu\n",
    "            buttons = driver.find_elements(by=By.CLASS_NAME, value=\"gsc-selected-option-container\")\n",
    "            buttons[0].click()\n",
    "\n",
    "            # change back to relevance\n",
    "            buttons =  driver.find_elements(by=By.CLASS_NAME, value=\"gsc-option-menu-item\")\n",
    "            buttons[0].click()\n",
    "            sleep(uniform(a,b))\n",
    "\n",
    "        end = time()\n",
    "\n",
    "        print(keyword, min(nums), f\"({end-start:.2f} seconds)\")\n",
    "\n",
    "        fo.write(f\"{keyword}, {min(nums)}\\n\")\n",
    "        fo.flush()\n",
    "        \n",
    "    except:\n",
    "        print(f\"The {keywords.index(keyword)}-th keyword {keyword} failed!\")\n",
    "        break\n",
    "\n",
    "# quit the driver\n",
    "driver.quit()\n",
    "\n",
    "# close the file\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8611c4e8-b782-4d0a-bf51-80d79faeb7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
